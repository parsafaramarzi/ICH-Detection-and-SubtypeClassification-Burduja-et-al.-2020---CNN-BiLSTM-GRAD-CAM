"""
# ICH-Detection-and-SubtypeClassification-Burduja-et-al.-2020---CNN-BiLSTM-GRAD-CAM

**Note:** This is a community implementation/fork of the architecture described in the paper. The **official repository** is maintained by the authors at: [https://github.com/warchildmd/ihd](https://github.com/warchildmd/ihd).

This repository implements the architecture proposed in the paper **"Accurate and Efficient Intracranial Hemorrhage Detection and Subtype Classification in 3D CT Scans with Convolutional and Long Short-Term Memory Neural Networks"** by Burduja et al. (2020).

The system is designed for patient-level classification of ICH and its five subtypes based on 3D CT scan volumes, utilizing a robust pipeline that includes fixed-length slice sampling and a combined CNN-BiLSTM network.

## ‚ö†Ô∏è Data Licensing and Compliance (IMPORTANT)

This repository contains only the **code** for the model architecture and training pipeline.

The raw CT scan data (`/data/ct_scans` and associated labels) are **restricted health data** governed by the **PhysioNet Restricted Health Data License**.

**Do not upload the dataset to this repository or any public platform.** Sharing the raw data violates the license agreement and patient privacy standards (e.g., HIPAA/GDPR).

To run this project, you must:

1.  Register and agree to the license terms on the official PhysioNet/Challenge website.
2.  Download the full dataset locally and place the files in the `/data` directory as described below.

## üß† Architecture Deep Dive: CNN-BiLSTM Hybrid Model

The core model, `FullModel`, is a sequential deep neural network that processes 3D volumes slice-by-slice. The solution involves a two-stage, two-loss hybrid architecture to efficiently process 3D volumes as a sequence of 2D images.

### Stage 1: Slice-Level Feature Extraction (CNN)

* **Component:** A standard CNN backbone (e.g., a pre-trained ResNet, adapted for single-channel grayscale input).
* **Function:** Processes individual CT slices to produce a feature embedding. This output also calculates an **auxiliary slice-level loss** (Binary Cross-Entropy), which acts as a regularizer, forcing the CNN to learn clinically relevant slice features.
* **Input/Output:** A single pre-processed 2D CT slice generates a fixed-size feature vector (slice feature embedding).


### Stage 2: Scan-Level Sequence Modeling (Bi-LSTM)

* **Component:** A Bidirectional Long Short-Term Memory (Bi-LSTM) network.
* **Function:** It captures the temporal/spatial context of the hemorrhage across the entire volume by processing the sequence of feature embeddings from the CNN in both forward and backward directions.
* **Input/Output:** A sequence of feature embeddings generated by the CNN is processed, yielding the final patient-level diagnosis (ICH presence and subtypes) via a fully-connected layer.

### Loss Function

The total loss is a weighted sum of the auxiliary slice-level loss and the primary scan-level loss:

$$\text{Loss}_{\text{Total}} = (1 - \lambda) \cdot \text{Loss}_{\text{Scan}} + \lambda \cdot \text{Loss}_{\text{Slice}}$$

The loss uses a weighted Binary Cross-Entropy (BCE) approach to account for class imbalance, where $\lambda$ is the `AUXILIARY_LOSS_WEIGHT` (default 0.5).

## 3. Data Pipeline Implementation: Fixed-Length Sampling

The data loading and sampling, managed by the `HemorrhageDataset` class and the `data_load` function, is critical due to the variable number of slices per patient.

### Pre-processing and Windowing

1.  **Hounsfield Units (HU) Windowing:** CT scan volumes are loaded from NIfTI files (`.nii`). The pixel values are normalized using various clinical windows (e.g., Brain, Subdural) to highlight specific tissue densities, ensuring consistency for the CNN input.
2.  **Resizing:** Each slice is resized to a uniform `IMG_SIZE` (e.g., 256x256).

### Fixed-Length Slice Sampling

* **Problem:** Patients have a variable number of slices ($N$), but the Bi-LSTM requires a fixed sequence length ($S$) for batching (e.g., 16 slices).
* **Solution:** For every patient, the loader selects exactly $S$ slices by sampling them uniformly across the volume's depth (the z-axis). This ensures the model receives a consistent, representative, and fixed-size input sequence regardless of the original scan size, which prevents the `RuntimeError: Trying to resize storage that is not resizable` crash during batching.

## üõ†Ô∏è Setup and Installation

### Prerequisites

* Python 3.8+
* PyTorch (Currently configured for CPU-only training)

### Dependencies

Install the necessary Python libraries:

```bash
pip install torch torchvision torchaudio numpy pandas scikit-learn nibabel opencv-python albumentations
```

### Data Structure

Once the data is downloaded, the project expects the input data to be organized as follows:

```
/data
‚îú‚îÄ‚îÄ hemorrhage_diagnosis_raw_ct.csv   # Master CSV with slice-level labels
‚îî‚îÄ‚îÄ ct_scans/
    ‚îú‚îÄ‚îÄ 001.nii                      # NIfTI file for Patient 1 (3D Volume)
    ‚îú‚îÄ‚îÄ 002.nii
    ‚îî‚îÄ‚îÄ ...
```

## üöÄ Training

The `src/train.py` script executes the entire training pipeline.

### Execution

To begin training:

```bash
python src/train.py
```

### Key Training Parameters (in `src/train.py`)

| Parameter | Default Value | Description |
| :--- | :--- | :--- |
| `BATCH_SIZE` | `4` | Number of patients per batch. |
| `IMG_SIZE` | `256` | Target pixel size for slices. |
| `EPOCHS` | `10` | Total training epochs. |
| `AUXILIARY_LOSS_WEIGHT` | `0.5` | Weight ($\lambda$) for the auxiliary slice-level loss. |
"""
